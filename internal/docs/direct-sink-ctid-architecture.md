# Direct Sink CTID-Based Architecture Plan

## Overview

Complete redesign of Direct Sink using CTID-based copy operations inspired by PeerDB's proven approach, with clean separation between Copy and Stream phases.

## Key Principles

- **Pure bulk copy** (no transaction logic)
- **Size-based rotation** (128MB files)
- **Resumable operations** (CTID-based progress tracking)
- **Clean separation** (copy vs stream)
- **No data loss** (snapshot consistency)
- **Independent components** (no fallbacks needed)
- **High performance** (CTID chunking, no LIMIT/OFFSET)

## Architecture Components

### 1. CopyReplicator

**Purpose**: Bulk data transfer using CTID-based chunking
**Key Features**:

- PostgreSQL snapshot export for consistency
- CTID range queries for performance (`WHERE ctid BETWEEN '(0,1)' AND '(100,1)'`)
- 2000 rows per chunk (configurable)
- Consistent parquet file rotation (global 128MB limit)
- Resumable operations with exact CTID tracking

```go
type CopyReplicator struct {
    config         *Config
    conn           *pgx.Conn
    parquetWriter  *BulkParquetWriter
    progressStore  CopyProgressStore
    snapshotInfo   *SnapshotInfo
    chunkSize      int               // Default: 2000 rows
    maxFileSize    int64             // Default: 128MB
}

type CopyProgress struct {
    TableName     string
    LastCTID      string    // "(150,32)" - exact resumption point
    BytesWritten  int64     // For file rotation tracking
    FileCount     int       // For naming: users_copy_001.parquet
    Status        CopyStatus // NOT_STARTED, IN_PROGRESS, COMPLETED
    SnapshotName  string    // For consistency
    StartedAt     time.Time
    CompletedAt   *time.Time
}

type SnapshotInfo struct {
    SnapshotName string
    LSN          pglogrepl.LSN
    ExportedAt   time.Time
}
```

### 2. StreamReplicator

**Purpose**: Real-time CDC with transaction boundaries
**Key Features**:

- WAL streaming from copy handoff LSN
- Transaction consolidation with commit lock pattern
- Size-based file rotation (128MB limit) with transaction boundary awareness
- CDC metadata columns (`_pg_flo_*`, `_old_*`)

**Important: File Rotation vs Transaction Boundaries**:

- **File Rotation**: Size-based (128MB), happens independently for optimal performance
- **Transaction Boundaries**: Commit lock ensures data written only after COMMIT
- **Both concepts work together**: Files rotate on size, but respect transaction boundaries

```go
type StreamReplicator struct {
    config          *Config
    replicationConn *ReplicationConnection
    parquetWriter   *CDCParquetWriter
    consolidator    *OperationConsolidator
    commitLock      *CommitLock
    transactionStore TransactionStore
}
```

### 3. DirectReplicator (Orchestrator)

**Purpose**: Coordinate Copy â†’ Stream transition

```go
type DirectReplicator struct {
    copyReplicator   *CopyReplicator
    streamReplicator *StreamReplicator
    metadataStore    MetadataStore
    config          *Config
}

func (dr *DirectReplicator) Execute() error {
    // Phase 1: Copy with snapshot consistency
    snapshotInfo, err := dr.copyReplicator.StartCopy()
    if err != nil { return err }

    // Phase 2: Stream from snapshot LSN
    return dr.streamReplicator.StartFromLSN(snapshotInfo.LSN)
}
```

## File Organization

### Copy Phase Files (Size-Based Rotation Only)

```
/data/copy/
â”œâ”€â”€ users_copy_20250825_001.parquet      (128MB - bulk data, no CDC metadata)
â”œâ”€â”€ users_copy_20250825_002.parquet      (64MB - remainder, no CDC metadata)
â””â”€â”€ transactions_copy_20250825_001.parquet (128MB - bulk data, no CDC metadata)
```

### Stream Phase Files (Size-Based + Transaction Boundary Aware)

```
/data/stream/
â”œâ”€â”€ users_stream_20250825_082631.parquet      (128MB rotated, includes CDC metadata)
â”œâ”€â”€ users_stream_20250825_082745.parquet      (Next 128MB chunk, includes CDC metadata)
â””â”€â”€ transactions_stream_20250825_082631.parquet (Per-table files, transaction boundaries respected)
```

**Key Differences**:

- **Copy**: Pure size-based rotation, no transaction logic, no CDC metadata
- **Stream**: Size-based rotation BUT respects transaction boundaries, includes CDC metadata

## Metadata Store Schema - Consolidated Design

**Inspired by PeerDB's proven approach**: Minimal tables, batched updates, monotonic progress tracking

### Core Tables

#### `replication_state` - Stream Progress Tracking

```sql
CREATE TABLE pgflo_metadata.replication_state (
    group_name TEXT PRIMARY KEY,
    last_lsn TEXT NOT NULL DEFAULT '0/0',
    sync_batch_id BIGINT DEFAULT 0,      -- Batch completion tracking (PeerDB pattern)
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Key Features:**

- âœ… **Batched Updates**: LSN updated every N commits or time interval (not per transaction)
- âœ… **Monotonic Progress**: Uses `GREATEST()` to prevent LSN regression
- âœ… **Batch Tracking**: `sync_batch_id` tracks completion of parquet file batches

#### `copy_state` - Consolidated Copy Progress

```sql
CREATE TABLE pgflo_metadata.copy_state (
    group_name      VARCHAR(255) NOT NULL,
    table_name      VARCHAR(255) NOT NULL,
    last_ctid       VARCHAR(50),           -- "(150,32)" - exact resumption point
    bytes_written   BIGINT DEFAULT 0,      -- For file rotation tracking
    file_count      INTEGER DEFAULT 0,     -- For naming: users_copy_001.parquet
    status          VARCHAR(20) DEFAULT 'NOT_STARTED', -- NOT_STARTED, IN_PROGRESS, COMPLETED
    snapshot_lsn    TEXT,                  -- Consolidated from copy_snapshots
    started_at      TIMESTAMP WITH TIME ZONE,
    completed_at    TIMESTAMP WITH TIME ZONE,
    updated_at      TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    PRIMARY KEY (group_name, table_name)
);
```

**Consolidation Benefits:**

- âŒ **Removed**: `copy_snapshots` (merged into `copy_state.snapshot_lsn`)
- âŒ **Removed**: `copy_progress` (consolidated into `copy_state`)
- âœ… **Simplified**: One table for all copy tracking per table
- âœ… **Resumable**: CTID-based resumption with snapshot LSN for handoff

#### `s3_files` - Future S3 â†’ Redshift Pipeline

```sql
CREATE TABLE pgflo_metadata.s3_files (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    file_path TEXT NOT NULL UNIQUE,
    sync_batch_id BIGINT NOT NULL,       -- Links to replication_state.sync_batch_id
    table_names TEXT[] NOT NULL,
    file_size_bytes BIGINT,
    row_count BIGINT,
    processed_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Future Pipeline Support:**

- âœ… **Batch Linking**: `sync_batch_id` links parquet files to replication batches
- âœ… **Redshift Workers**: Track S3 files for downstream sync workers
- âœ… **Processing State**: Mark files as processed after Redshift sync

## Batching Strategy - Anti-Thrashing Design

**Inspired by PeerDB's proven approach**: Avoid per-transaction metadata updates

### Stream Phase Batching

```go
// Current (thrashing): Update LSN on every commit
func (sr *StreamReplicator) handleCommitMessage(msg *pglogrepl.CommitMessage) error {
    // Process transaction...
    return sr.saveStreamingState(msg.CommitLSN) // âŒ Called 1000x/sec
}

// New (batched): Update LSN periodically
const (
    CHECKPOINT_BATCH_SIZE = 100          // Every 100 commits
    CHECKPOINT_INTERVAL   = 5 * time.Second // Or every 5 seconds
)

func (sr *StreamReplicator) handleCommitMessage(msg *pglogrepl.CommitMessage) error {
    // Process transaction...
    sr.commitCount++
    sr.lastProcessedLSN = msg.CommitLSN

    // Batch checkpoint updates (PeerDB pattern)
    if sr.commitCount%CHECKPOINT_BATCH_SIZE == 0 ||
       time.Since(sr.lastCheckpoint) > CHECKPOINT_INTERVAL {
        return sr.saveStreamingCheckpoint(msg.CommitLSN, sr.currentBatchID)
    }
    return nil
}
```

### Monotonic LSN Updates (PeerDB Pattern)

```go
func (ms *PostgresMetadataStore) UpdateStreamingLSN(ctx context.Context, groupName string, lsn pglogrepl.LSN, batchID int64) error {
    _, err := ms.pool.Exec(ctx, `
        INSERT INTO pgflo_metadata.replication_state (group_name, last_lsn, sync_batch_id)
        VALUES ($1, $2, $3)
        ON CONFLICT (group_name) DO UPDATE SET
            last_lsn = GREATEST(replication_state.last_lsn, excluded.last_lsn),  -- Prevent regression
            sync_batch_id = GREATEST(replication_state.sync_batch_id, excluded.sync_batch_id),
            updated_at = NOW()`,
        groupName, lsn.String(), batchID)
    return err
}
```

### Parquet File â†’ S3 â†’ Redshift Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Direct Sink   â”‚    â”‚ Local Parquetâ”‚    â”‚   S3 Storage    â”‚    â”‚   Redshift      â”‚
â”‚                 â”‚    â”‚   Files      â”‚    â”‚                 â”‚    â”‚   Workers       â”‚
â”‚                 â”‚    â”‚              â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Copy Phase    â”‚â”€â”€â”€â–¶â”‚ /copy/*.pqt  â”‚â”€â”€â”€â–¶â”‚ s3://bucket/    â”‚â”€â”€â”€â–¶â”‚ COPY FROM S3    â”‚
â”‚ â€¢ Stream Phase  â”‚    â”‚ /stream/*.pqtâ”‚    â”‚   copy/         â”‚    â”‚ â€¢ Batch loading â”‚
â”‚ â€¢ Batch LSN     â”‚    â”‚              â”‚    â”‚   stream/       â”‚    â”‚ â€¢ State trackingâ”‚
â”‚   tracking      â”‚    â”‚              â”‚    â”‚                 â”‚    â”‚ â€¢ Error handlingâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                       â”‚                       â”‚
                              â–¼                       â–¼                       â–¼
                    sync_batch_id                sync_batch_id         sync_batch_id
                    (in filename)                (in s3_files)        (completion tracking)
```

**Key Benefits:**

- âœ… **Reduced Metadata I/O**: 100x fewer database writes (10/sec vs 1000/sec)
- âœ… **Resumability**: Max 5-second data loss on restart (vs per-transaction)
- âœ… **Batch Traceability**: Files linked to sync batches for Redshift workers
- âœ… **Monotonic Progress**: Never regress LSN even with concurrent workers

### Why Not Temporal? - Simple is Better

**PeerDB uses Temporal for complex multi-destination CDC**. For Direct Sink's focused use case, we get PeerDB's proven patterns WITHOUT the complexity:

```
PeerDB (Complex):                    pg_flo Direct Sink (Focused):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Temporal Workflows  â”‚             â”‚   Simple Go App     â”‚
â”‚ â€¢ Multiple Workers  â”‚             â”‚ â€¢ Single Binary     â”‚
â”‚ â€¢ Activity Retries  â”‚             â”‚ â€¢ Built-in Retries  â”‚
â”‚ â€¢ State Machines    â”‚             â”‚ â€¢ PostgreSQL State  â”‚
â”‚ â€¢ Distributed Sync  â”‚             â”‚ â€¢ Localâ†’S3â†’Redshift â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                    â”‚
         â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Multiple Destinationsâ”‚             â”‚ Parquet â†’ S3 Only  â”‚
â”‚ â€¢ BigQuery         â”‚             â”‚ â€¢ Optimized Path    â”‚
â”‚ â€¢ Snowflake        â”‚             â”‚ â€¢ Minimal Overhead  â”‚
â”‚ â€¢ ClickHouse       â”‚             â”‚ â€¢ Direct Control    â”‚
â”‚ â€¢ Postgres         â”‚             â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Direct Sink Advantages:**

- âœ… **Simpler Operations**: No Temporal cluster to manage
- âœ… **Lower Latency**: No workflow scheduling overhead
- âœ… **Direct Control**: Custom retry/recovery logic
- âœ… **Focused Design**: Optimized for Parquetâ†’S3â†’Redshift pipeline
- âœ… **Proven Patterns**: Adopts PeerDB's metadata batching without complexity

## Implementation Plan

### Phase 1: Core Components

1. âœ… Create CTID-based CopyReplicator
2. âœ… Implement BulkParquetWriter (no CDC metadata)
3. âœ… Build CopyProgressStore with metadata tables
4. âœ… Add snapshot export/consistency logic

### Phase 2: Stream Integration

1. âœ… Create clean StreamReplicator
2. âœ… Implement CDC ParquetWriter (with metadata)
3. âœ… Build snapshot â†’ stream handoff
4. âœ… Update DirectReplicator orchestration

### Phase 3: Testing & Validation

1. âœ… Update E2E tests for copy/stream separation
2. âœ… Add unit tests for CopyReplicator and StreamReplicator
3. âœ… Validate file rotation and resumption
4. âœ… Performance testing with large datasets

## Migration Strategy

### Backward Compatibility

- Keep existing `direct_replicator.go` as fallback initially

### Configuration Changes

```yaml
# Global parquet file configuration
max_parquet_file_size: 134217728 # 128MB in bytes (consistent across copy & stream)

# Copy phase configuration
copy:
  chunk_size: 2000 # Rows per CTID chunk
  # No transaction logic, consolidation, or commit locks - just bulk copy with size rotation

# Stream phase configuration (inspired by PeerDB batching)
stream:
  checkpoint_batch_size: 100 # Update LSN every N commits (anti-thrashing)
  checkpoint_interval: "5s" # Or every 5 seconds (whichever comes first)
  # All built-in behavior:
  # - Size-based rotation (128MB) with transaction boundary awareness
  # - Commit lock pattern (data written only after COMMIT)
  # - Operation consolidation (key-based deduplication)
  # - Transaction store with disk spilling
  # - Batched metadata updates (PeerDB pattern)

# Metadata configuration
metadata:
  # Consolidated schema with minimal tables
  # - replication_state: LSN + batch tracking
  # - copy_state: Consolidated copy progress
  # - s3_files: Future S3 â†’ Redshift pipeline
```

## Testing Strategy

### Unit Tests

- `CopyReplicator` CTID chunking logic
- `CopyProgressStore` metadata operations
- `BulkParquetWriter` file rotation
- `StreamReplicator` transaction handling
- Snapshot export/import consistency

### Integration Tests

- Copy â†’ Stream handoff accuracy
- Resumption after interruption
- Large table handling (1M+ rows)
- Multi-table coordination

### E2E Tests

- Update `e2e_direct_sink.sh` for copy/stream validation
- Separate validation for copy files vs stream files
- File count and size validation
- Data completeness verification

## Monitoring & Observability

- Copy progress metrics (bytes/rows processed)
- File rotation events and sizes
- CTID chunk processing times
- Snapshot consistency validation
- Stream handoff accuracy

## Risk Mitigation

- **CTID limitations**: Fallback to row-based for compressed tables
- **Snapshot timeout**: Configurable snapshot maintenance duration
- **File corruption**: Checksums and validation on write
- **Memory pressure**: Configurable chunk sizes and batch limits
- **Network issues**: Retry logic with exponential backoff

## Success Criteria

- âœ… 100% data consistency between source and parquet
- âœ… Sub-second resumption for interrupted operations
- âœ… Zero data loss during copy â†’ stream transition
- âœ… Clean separation of concerns (no NATS dependencies)
- âœ… All existing E2E tests pass with new architecture

## Session Restart Information

### Project Structure & Key Files

**Core Implementation Files:**

- `/Users/shayon/src/pg_flo/pkg/direct/copy_replicator.go` - âœ… CTID-based copy logic
- `/Users/shayon/src/pg_flo/pkg/direct/metadata_store.go` - âœ… Enhanced with copy progress tracking
- `/Users/shayon/src/pg_flo/pkg/direct/interfaces.go` - âœ… Updated with new methods
- `/Users/shayon/src/pg_flo/pkg/direct/direct_replicator.go` - Current orchestrator (needs refactoring)
- `/Users/shayon/src/pg_flo/pkg/direct/parquet_writer.go` - Current writer (needs separation)

**Configuration & Documentation:**

- `/Users/shayon/src/pg_flo/internal/docs/direct-sink-ctid-architecture.md` - This plan document
- `/Users/shayon/src/pg_flo/pkg/direct/types.go` - Config structures
- `/Users/shayon/src/pg_flo/internal/direct-sink.yaml` - Config template

**Test Files:**

- `/Users/shayon/src/pg_flo/internal/scripts/e2e_direct_sink.sh` - E2E test script
- `/Users/shayon/src/pg_flo/internal/scripts/e2e_test_local.sh` - E2E runner

### Essential Commands

**Build Project:**

```bash
cd /Users/shayon/src/pg_flo
go build -o bin/pg_flo
```

**Run Tests:**

```bash
# Unit tests
make test

# Linting
make lint

# E2E Test (Direct Sink)
E2E_TEST=e2e_direct_sink.sh ./internal/scripts/e2e_test_local.sh

# Monitor E2E logs
tail -f /tmp/pg_flo*
```

**Test File Inspection:**

```bash
# Check Parquet files (new parquet-tools)
parquet-tools inspect /tmp/pg_flo_direct_parquet/public.users_*.parquet
parquet-tools show --head 10 /tmp/pg_flo_direct_parquet/public.users_*.parquet

# Check file sizes and counts
ls -la /tmp/pg_flo_direct_parquet/
```

### Current Status Summary

**âœ… COMPLETED:**

1. **Metadata Store Enhanced** - Added `copy_progress`, `copy_snapshots` tables
2. **CopyReplicator Implementation** - CTID-based chunking with snapshot consistency
3. **Interface Updates** - New methods for copy progress tracking
4. **Build & Test Pipeline** - All `make test` passing

**ğŸ”„ IN PROGRESS:**

1. **Code Cleanup** - Remove unused architecture components

**ğŸ“‹ PENDING (Priority Order):**

1. **âœ… Metadata Schema Consolidation** - Merge copy_progress + copy_snapshots â†’ copy_state
2. **ğŸ”„ Batched LSN Updates** - Implement PeerDB-style checkpoint batching (100 commits/5s)
3. **ğŸ”„ Monotonic LSN Progress** - Add GREATEST() pattern to prevent LSN regression
4. **ğŸ“‹ S3 Pipeline Preparation** - Add sync_batch_id linking for future Redshift workers
5. **ğŸ“‹ E2E Test Updates** - Validate consolidated metadata and reduced thrashing
6. **ğŸ“‹ Configuration Updates** - Add checkpoint_batch_size and checkpoint_interval
7. **ğŸ“‹ Unit Tests** - Test batching logic and metadata consolidation

### Key Architecture Decisions Made

1. **CTID-Based Copy** - Using PostgreSQL's physical row identifiers for efficient chunking
2. **Snapshot Consistency** - `pg_export_snapshot()` for point-in-time copy accuracy
3. **Clean Separation** - No NATS dependencies, distinct Copy vs Stream phases
4. **Size-Based Rotation** - 128MB parquet files for optimal performance
5. **Resumable Operations** - Exact CTID progress tracking for interruption recovery

### Current E2E Test Behavior

The E2E test currently uses the existing `direct_replicator.go` which shows:

- Copy phase working (some data copied)
- Stream phase working (CDC operations processing)
- Parquet files being written with transaction consolidation
- File validation using new `parquet-tools inspect` and `show` commands

### Next Session Steps

1. Implement `BulkParquetWriter` (copy phase, no CDC metadata)
2. Implement `StreamReplicator` (stream phase, with transaction logic)
3. Create separate `CDCParquetWriter` for stream metadata
4. Refactor `DirectReplicator` to orchestrate both phases
5. Update E2E tests to validate the new architecture
